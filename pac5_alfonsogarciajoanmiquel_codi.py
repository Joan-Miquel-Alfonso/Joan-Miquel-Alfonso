# -*- coding: utf-8 -*-
"""PAC5_AlfonsoGarciaJoanMiquel_codi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L13HeCxGmSGCzDekIIoFTwBNTsrSgk3I

Autor: Joan Miquel Alfonso Garcia

Aquest arxiu està preparat per ser emprat en Google Colaboratory (canviar el PATH pel que corresponga), en cas d'emprar Jupyter notebook, ignorar el primer codi i posar allí la ruta on estiga l'arxiu csv.
"""

from google.colab import drive
drive.mount('/content/drive')

PATH="/content/drive/My Drive/Analitica_de_clients/PAC 5/"

# Carreguem el dataset
import pandas as pd
pd.set_option('display.max_columns', None)
data=pd.read_csv(PATH+'data_r5_2024.csv')
data

"""Activitat 1: Anàlisi de les dades (25%)

Per a aquesta activitat es recomana utilitzar el paquet NLTK (Natural Language Toolkit) de Python.

a)	Explora el dataset, quantes mostres té? Quines variables tenen valors nuls? Realitza els procediments adequats per tenir un dataset amb informació constructiva.

"""

# Mostrem el nombre de mostres i columnes
print('El dataset conté', data.shape[0], 'files i ' , data.shape[1], 'columnes')

# Mostrem quin tipus de dada té cada variable
data.dtypes

# Mostrem el nombre de nulls per columna
data.isnull().sum()

# Mostrem el nombre de nulls per columna quan l'airline_sentiment és negatiu
data[data['airline_sentiment'] == 'negative'].isnull().sum()

"""b)	Divideix els tuits en tres grups: positius, negatius i neutres, realitza un gràfic de barres de sentiments dels usuaris per cada aerolínia. Quines aerolínies tenen comentaris més negatius? I comentaris més positius? Observant els gràfics, què dedueixes dels tuits negatius?"""

import plotly.express as px
# Agrupem per aerolínia i sentiment i comptem
sentiment_counts = data.groupby(['airline', 'airline_sentiment']).size().reset_index(name='counts')

# Creem un gràfic de barres
fig = px.bar(sentiment_counts, x='airline', y='counts', color='airline_sentiment', barmode='group',
             title='Tipus de Sentiments per Aerolínia', labels={'counts': 'Nombre de Tweets', 'airline': 'Aerolínies', 'sentiment': 'Sentiment'})

fig.show()

# Filtrem per positiu i negatiu
positive_counts = sentiment_counts[sentiment_counts['airline_sentiment'] == 'positive'].sort_values(by='counts', ascending=False)
negative_counts = sentiment_counts[sentiment_counts['airline_sentiment'] == 'negative'].sort_values(by='counts', ascending=False)

# Mostrem l'aerolínia amb més positius i l'aerolínia amb més negatius
most_positive_airline = positive_counts.iloc[0]['airline']
most_negative_airline = negative_counts.iloc[0]['airline']

print("L'aerolínia amb més twits positius és:",most_positive_airline, "| L'aerolínia amb més twits negatius és:", most_negative_airline)

# Comptem el total de tweets de cada aerolínia
total_counts = data.groupby('airline').size().reset_index(name='total_counts')

# Agrupem el total en el dataset que recompta els tweets per aerolínia i sentiment
sentiment_counts = sentiment_counts.merge(total_counts, on='airline')

# Calculem el percentatge
sentiment_counts['percentage'] = (sentiment_counts['counts'] / sentiment_counts['total_counts']) * 100

# Creem un gràfic de barres
fig = px.bar(sentiment_counts, x='airline', y='percentage', color='airline_sentiment', barmode='group',
             title='Percentatge de cadascun dels tipus de Sentiments per Aerolínia', labels={'percentage': 'Percentatge de Tweets', 'airline': 'Aerolínia', 'airline_sentiment': 'Sentiment'})

fig.show()

"""c)	Per a tenir una idea de les paraules més freqüents en els tuits negatius i positius, representa un núvol de paraules amb el paquet wordcloud de Python. Quines són les paraules més freqüents en tuits amb sentiments negatius? I en els positius?"""

# Encara que no ho demana l'exercici filtrem les paraules, eliminant els hagstags, signes de puntuació, posem totes en minúscula i eliminem els stopwords.
import nltk
import matplotlib.pyplot as plt
from wordcloud import WordCloud
# Descarruguem stopwords de la llibreria nltk
nltk.download('stopwords')
# Seleccionem els tweets negatius
negative = data[data['airline_sentiment'] == 'negative']
# Resetegem l'índex
negative_texts = negative['text'].reset_index()
# Ho passem a dataframe i seleccionem la columna text
dataframe_negative = pd.DataFrame(negative_texts, columns=['text'])
# Creem un diccionari
dictionary_negative = {}
# Fem un loop per passar per cada filera del dataframe
for y in range(dataframe_negative.shape[0]):
  # Fem un altre loop per passar per cadascuna de les paraules de la columna text
  for x in dataframe_negative['text'][y].split():
    # Eliminem els signes de puntuació
    x = x.replace('.','')
    x = x.replace(',','')
    x = x.replace('-','')
    # Fem totes les lletres minúscules
    x = x.lower()
    # Si la paraula no està al diccionari, no és una stopword ni un hagstag, l'afegim al diccionari
    if x not in dictionary_negative.keys() and x not in nltk.corpus.stopwords.words('english') and not x.startswith('@'):
      dictionary_negative[x] = 1
    # Si la paraula està al diccionari, li sumem més 1
    elif x not in nltk.corpus.stopwords.words('english') and not x.startswith('@'):
      dictionary_negative[x] += 1

# Creem el WorldCloud
wc = WordCloud(background_color='white').generate_from_frequencies(dictionary_negative)
plt.figure(figsize=(10, 10))
plt.imshow(wc)
plt.axis('off')
plt.show()

# Mostrem una llista de les parauales més freqüents
dict(sorted(dictionary_negative.items(), key=lambda item: item[1], reverse=True))

# Fem exactament el mateix però amb els tweets positius
positive = data[data['airline_sentiment'] == 'positive']
positive = positive['text'].reset_index()
dataframe_positive = pd.DataFrame(positive, columns=['text'])
dictionary_positive = {}
for y in range(dataframe_positive.shape[0]):
  for x in dataframe_positive['text'][y].split():
     x = x.replace('.','')
     x = x.replace(',','')
     x = x.replace('-','')
     x = x.lower()
     if x not in dictionary_positive.keys() and x not in nltk.corpus.stopwords.words('english') and not x.startswith('@'):
      dictionary_positive[x] = 1
     elif x not in nltk.corpus.stopwords.words('english') and not x.startswith('@'):
      dictionary_positive[x] += 1

wc = WordCloud(background_color='white').generate_from_frequencies(dictionary_positive)
plt.figure(figsize=(10, 10))
plt.imshow(wc)
plt.axis('off')
plt.show()

dict(sorted(dictionary_positive.items(), key=lambda item: item[1], reverse=True))

"""d)	Extreu conclusions sobre les raons dels sentiments negatius dels tuits dels clients representant la distribució de la variable 'negativereason' per cada aerolínia. Quina és la raó que genera més sentiments negatius? Destacaries alguna aerolínia? Justifica la teva resposta."""

# Agrupem per aerolínia i raó negativa
negativereason_counts = negative.groupby(['airline', 'negativereason']).size().reset_index(name='counts')

# Creem que el gràfic de barres
fig = px.bar(negativereason_counts, x='airline', y='counts', color='negativereason', barmode='group',
             title='Tipus de Sentiments per Aerolínia', labels={'counts': 'Nombre de Tweets', 'airline': 'Aerolínies', 'sentiment': 'Sentiment'})

fig.show()

"""e)	Existeix alguna relació entre els sentiments negatius dels usuaris en les aerolínies i la data? El dataset té registres des de 2015-feb-17 fins a 2015-feb-24, justifica la teva resposta mitjançant la visualització en un gràfic de barres de la distribució de tuits negatius en cada aerolínia per cada data. Quines conclusions obtens?"""

# Fem la columna tweet_created com datetime
negative.loc[:, 'tweet_created'] = pd.to_datetime(negative['tweet_created']).dt.date

# Agrupem per data i aerolínia i comptem
negative_counts_by_day = negative.groupby(['tweet_created', 'airline']).size().reset_index(name='counts')

# Creem el gràfic de barres
fig = px.bar(negative_counts_by_day, x='tweet_created', y='counts', color='airline', barmode='group',
             title='Distribució de Tweets Negatius per Aerolínea i Data',
             labels={'tweet_created': 'Data', 'counts': 'Nombre de Tweets Negatius', 'airline': 'Aerolínia'})

fig.show()

"""Activitat 2: Preprocesamiento de text (30%)

La part més important del NLP és el preprocessament de text per passar d'un llenguatge lliure a un format òptim en màquina per al seu posterior processament.
Atès que hi ha molts paràmetres innecessaris en el conjunt de dades, ens quedarem amb les columnes  ‘airline_sentiment’, ‘tweet_id’ i el text del tuit.

Realitza els següents apartats sobre la columna ‘text’ i emmagatzema els resultats en una columna nova, ‘cleaned_text’:

f)	La primera implementació que farem es basarà a aplicar tokenització, és a dir, per a cada seqüència de caràcters, ens permet dividir en peces discretes anomenades tokens (1 paraula = 1 token). Normalment, aquest procés implica l'eliminació de certs caràcters, com els signes de puntuació.
Es recomana utilitzar la biblioteca NLTK que té un tokenitzador incorporat.

"""

# Seleccionem els tweets amb sentiment positiu o negatiu
data2 = data[data['airline_sentiment'].isin(['positive', 'negative'])]

# Seleccionem les columnes tweet_id, airline_sentiment i text
data2 = data2[['tweet_id','airline_sentiment','text']]

data2

import string
import re
from nltk.tokenize import word_tokenize
nltk.download('punkt')

# Fem una funció que elimina signes de puntuació, eliminem números, fem totes les paraules en minúscula i tokenitzem
def tokenize_and_remove_punctuations(s):
    for char in string.punctuation:
        s = s.replace(char, ' ')
    ss = ''.join([x for x in s if not x.isdigit()])
    return nltk.word_tokenize( ss.lower() )

# Apliquem la funció al nostre dataset per crear la columna cleaned_text
data2['cleaned_text'] = data2['text'].apply(tokenize_and_remove_punctuations)

data2

"""g)	Eliminem les Stop Words, que bàsicament són preposicions o adverbis que no ajuden a determinar la qualitat semàntica del tuit. Per a això, es recomana utilitzar el mòdul de corpus i importar la funció de stopwords."""

# Fem una funció per a eliminar les paraules amb 2 o menys lletres i les paraules Stop Words, ja que aquestes no tenen significat semàntic
def remove_stop_words(tokens):
    filtered_words= []
    for x in tokens:
      if x not in nltk.corpus.stopwords.words('english') and len(x) > 2:
        filtered_words.append(x)
    return filtered_words

data2['cleaned_text'] = data2['cleaned_text'].apply(remove_stop_words)

data2

"""h)	A continuació, heu de convertir totes les paraules en minúscules i eliminar els signes de puntuació que no aporten significat. Es recomana utilitzar el paquet re de Python.

A efectes gramaticals, els documents utilitzen diferents formes d'una paraula (look, looks, looking, looking) que en moltes situacions tenen qualitats semàntiques molt similars. El stemming redueix les paraules a la seva paraula stem o arrel, de ‘jumping, jumped’ seria ‘jump’. La lematització és l'eliminació de lletres amb prefix o sufix d'una paraula, el resultat pot ser o no una paraula del corpus lingüístic, de ‘am,are a ‘be’.

i)	Realitza una de les dues tècniques anteriors sobre les dades treballades. Per a fer això, es recomana utilitzar el mòdul de stem de NLTK que ofereix diversos algorismes. Justifica l'elecció de l'algorisme seleccionat.

Com a eina de suport us adjuntem un blog per manejar dades NLP i preprocessament de dades.

En base als resultats obtinguts, identifiques alguna paraula que no aporti significat al sentiment del tuit a primera vista? Què creus que pot estar passant amb les ortografies incorrectes?
"""

from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('wordnet')
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Fem una funció que lematitza les paraules
def function_lemmatizer(tokens):
    tokens = [lemmatizer.lemmatize(x) for x in tokens]
    return tokens

data2['cleaned_text'] = data2['cleaned_text'].apply(function_lemmatizer)

data2

"""j)	Una altre metodologia interessant per aquest tipus de cas d’ús són els anomenats Transformers de text, per exemple tenim els models BERT o GPT. Explica la principal diferència entre ells.

k)	A partir d'un BERT, 'bert-base-cased' (https://huggingface.co/bert-base-cased), computa els vectors representatius del text dels tweets i guarda'ls a una nova columna anomenada “embeddings_text”.
"""

import torch
from transformers import BertTokenizer, BertModel
from tqdm import tqdm

pd.set_option('display.max_colwidth', 1)

# Fem que ho faça amb cuda per a què vaja més ràpid
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Fem el model i el tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = BertModel.from_pretrained('bert-base-cased').to(device)



# Fem una funció per a computar els embeddings amb BERT
def compute_embeddings(tokens):
    # Tokenitzem el text
    tokens = tokenizer(tokens, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)
    # Obtenim els embeddings
    with torch.no_grad():
        outputs = model(**tokens)
    # Obtenim el vector del primer token
    embeddings = outputs.last_hidden_state[:, 0, :].squeeze().numpy()
    return embeddings

# Posem una barra de càrrega per saber l'estat del procés, ja que sol tardar prop de 30 minuts
tqdm.pandas()

# apliquem la funció als tweets originals, ja que BERT funciona millor en textos complets. (He comprovat fent-ho amb cleaned_text i obtenia pitjors resultats a l'exercici 3c)
data2['embeddings_text'] = data2['text'].progress_apply(compute_embeddings)


data2

"""Finalment, dividirem les dades en conjunt d'entrenament, test i validació per realitzar l'activitat 3 i poder classificar tuits en positius i negatius. Per a fer això es considera com a variable explicativa 'cleaned_text' pel primer cas d'ús, 'embeddings_text' pel segon cas d'ús  i com a variable objectiu pels dos casos d'ús 'airline_sentiment'.

l)	Donant-te suport en la funció train_test_split de scikit-learn, divideix les dades d'entrenament 70%, validació 10% i test 20%; un fitxer per a cada cas d'ús. Per tant, tindrem dos fitxers de train anomenats X_train_cas1, X_train_cas2…
Es recomana incloure random_state=126 perquè els resultats siguin reproduïbles.


"""

from sklearn.model_selection import train_test_split
import numpy as np


# En primer lloc, seleccionem la variable per a X_cas1 i per a X_cas2
X_cas1 = data2['cleaned_text']
X_cas2 = np.array(data2['embeddings_text'].tolist())
# Seleccionem la variable objectiu
y = data2['airline_sentiment']

# Dividim entre conjunt d'entrenament (70%) i conjunt de test i validació (30%)
X_train_cas1, X_tesval_cas1, y_train_cas1, y_tesval_cas1 = train_test_split(X_cas1, y, test_size=0.3, random_state=126)

# Dividim el conjunt anterior entre conjunt de validació (33%) i conjunt de test (66%), que correspondria al 10 i 20% del conjunt original
X_val_cas1, X_test_cas1, y_val_cas1, y_test_cas1 = train_test_split(X_tesval_cas1, y_tesval_cas1, test_size=0.66, random_state=126)

X_train_cas1, X_val_cas1, X_test_cas1

# Fem el mateix però amb el cas2
X_train_cas2, X_tesval_cas2, y_train_cas2, y_tesval_cas2 = train_test_split(X_cas2, y, test_size=0.3, random_state=126)

X_val_cas2, X_test_cas2, y_val_cas2, y_test_cas2 = train_test_split(X_tesval_cas2, y_tesval_cas2, test_size=0.66, random_state=126)

X_train_cas2, X_val_cas2, X_test_cas2

"""m)	La freqüència de termes o TF-IDF sol utilitzar-se per produir pesos associats a paraules que poden ser útils en cerques de recuperació d'informació. Utilitza la implementació de scikit-learn de TfidfVectorizer per convertir la col·lecció de tuits sense processar en una matriu de funcions TF-IDF sobre les mostres del primer cas d’ús, és a dir, sobre X_train_cas1,X_val_cas1,X_test_cas1."""

from sklearn.feature_extraction.text import TfidfVectorizer
# En primer lloc, fem els tokens com string i on cada paraula està separada per un espai en blanc
X_train_cas1 = X_train_cas1.apply(lambda tokens: ' '.join(tokens))
X_val_cas1 = X_val_cas1.apply(lambda tokens: ' '.join(tokens))
X_test_cas1 = X_test_cas1.apply(lambda tokens: ' '.join(tokens))


v = TfidfVectorizer()
# Ajustem el vectoritzer amb les dades transformem  en una matriu TF-IDF
X_train_cas1 = v.fit_transform(X_train_cas1)
# Transformem les dades en una matriu TF-IDF utilitzant el vectoritzador ja ajustat
X_val_cas1 = v.transform(X_val_cas1)
X_test_cas1 = v.transform(X_test_cas1)

# Comprovem que tots tenen el mateix nombre de columnes
print("Dimensions de X_train_cas1:", X_train_cas1.shape)
print("Dimensions de X_val_cas1:", X_val_cas1.shape)
print("Dimensions de X_test_cas1:", X_test_cas1.shape)

"""Activitat 3: Topic Modelling (35%)

Ja tenim les dades preparades per a poder entrenar els models predictius per classificar els tuits en positius i negatius.

Amb els fitxers del primer cas d’ús, realitza:

a)	Entrena un Logistic Regression, un Random Forest, un LinearSVC i un Gradient Boosting Classifier de la llibreria de sklearn amb els hiperparàmetres en default excepte el random state, que usarem el 126. Construeix la matriu de confusió per cada model amb el dataset de proves i contesta les següents preguntes:

i)	Quin model té millor accuracy?

"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.calibration import CalibratedClassifierCV

# Fem un diccionari amb cadascun dels models, per a Linear SVC emprem CalibratedClassifierCV per poder fer la corba ROC
models = {
    "Logistic Regression": LogisticRegression(random_state=126),
    "Random Forest": RandomForestClassifier(random_state=126),
    "Linear SVC": CalibratedClassifierCV(LinearSVC(random_state=126)),
    "Gradient Boosting": GradientBoostingClassifier(random_state=126)
}

# Entrenem i avaluem cadascun dels models
for name, model in models.items():
    print(f"Entrenament del model: {name}")
    model.fit(X_train_cas1, y_train_cas1)
    y_pred = model.predict(X_test_cas1)

    # Construim la matriu de confusió
    cm = confusion_matrix(y_test_cas1, y_pred, labels=model.classes_)

    # Mostrem la matriu de confusió
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f"Confusion Matrix for {name}")
    plt.show()

    # Fem i mostrem el report de classificació amb les mesures Accuracy, Precision, Recall i F1-score
    print(f"Classification Report for {name}:\n")
    print(classification_report(y_test_cas1, y_pred, target_names=model.classes_))
    # Calculem al probabilitat
    y_prob = model.predict_proba(X_test_cas1)
    # Binaritzem la variable objectiu
    y_test_cas1_bin = label_binarize(y_test_cas1, classes=np.unique(y_train_cas1))
    # Calculem el valor roc_auc
    roc_auc = roc_auc_score(y_test_cas1_bin, y_prob[:,1])
    print(f"ROC AUC Score for {name}: {roc_auc}\n")

    # Calculem el FPR i TPR
    fpr, tpr, _ = roc_curve(y_test_cas1_bin, y_prob[:,1])
    roc_auc = auc(fpr, tpr)

    # Dibuixem les corbes ROC
    plt.figure()
    plt.plot(fpr, tpr, lw=2,label=f'ROC curve of class {model.classes_})')

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f"ROC Curve for {name}")
    plt.show()

"""ii)	Quin té millor recall?

iii)	Implementa la Corba ROC per cada model. Explica amb les teves paraules quina informació ens proporciona aquesta mètrica.

iv)	Tria un model i implementa un Random Search (entre 2 i 5 paràmetres per cada array del grid) per trobar una combinació millor respecte al default dels hiperparàmetres. Realitza la matriu de confusió. Quin % de millora s'ha guanyat d'accuracy i recall si ho comparem amb la versió anterior?
"""

from sklearn.model_selection import RandomizedSearchCV


# Definim els hiperparàmetres per al Random Search
param_distributions = {
    'C': [0.01, 0.1, 1, 10, 100],
    'max_iter': [1000, 2000, 3000, 4000, 5000],
    'tol': [1e-4, 1e-3, 1e-2, 1e-1]
}

# Creem el model LinearSVC, ja que és el que obté millor resultats
svc = LinearSVC(random_state=126)

# Configurem el RandomizedSearchCV
random_search = RandomizedSearchCV(estimator=svc, param_distributions=param_distributions,
                                   n_iter=50, cv=3, verbose=2, random_state=126, n_jobs=-1)

# Entrenem el RandomizedSearchCV
random_search.fit(X_train_cas1, y_train_cas1)

# Mostrem el millor conjunt d'hiperparàmetres
print("Best Hyperparameters:", random_search.best_params_)

# Entrenem el model amb la millor combinació d'hiperparàmetres
best_svc = random_search.best_estimator_
best_svc.fit(X_train_cas1, y_train_cas1)

y_pred = best_svc.predict(X_test_cas1)

cm_optimized = confusion_matrix(y_test_cas1, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Optimized Linear SVC')
plt.show()

print(classification_report(y_test_cas1, y_pred, target_names=model.classes_))

"""b)	En les dades tenim una feature que ha estat computada amb l'objectiu de saber el perquè del sentiment negatiu.

i)	Partint del dataset de training, crea un de nou amb només els tuits negatius i amb el dataset de test realitza el mateix per després poder comprovar el bé que funciona el model.

"""

# Convertim la matriu sparse en un dataset
X_test_cas1_df = pd.DataFrame.sparse.from_spmatrix(X_test_cas1, index=y_test_cas1.index)

# Combinem el Dataframe de la variable explicativa amb el dataframe de la variable objetiu
X_test = X_test_cas1_df.join(y_test_cas1)

X_test

# Seleccionem solament els tweets de sentiment negatiu
X_test = X_test[X_test['airline_sentiment'] == 'negative']

X_test

# Eliminem la variable airline_sentiment
X_test = X_test.drop('airline_sentiment', axis = 1)
# Juntem aquest dataset amb la variable negativereason del dataset original (es junten seguint l'index)
X_test = X_test.join(data['negativereason'])

X_test

# Seleccionem negative reason com la variable objectiu
y_test = X_test['negativereason']

y_test

# Eliminem negativereason de les variables explicatives
X_test = X_test.drop('negativereason', axis = 1)

X_test

# Fem el mateix però amb el conjunt d'entrenament
X_train_cas1_df = pd.DataFrame.sparse.from_spmatrix(X_train_cas1, index=y_train_cas1.index)

X_train = X_train_cas1_df.join(y_train_cas1)

X_train

X_train = X_train[X_train['airline_sentiment'] == 'negative']

X_train

X_train = X_train.drop('airline_sentiment', axis = 1)
X_train = X_train.join(data['negativereason'])

X_train

y_train = X_train['negativereason']

y_train

X_train = X_train.drop('negativereason', axis = 1)

X_train

"""ii)	Realitza un petit estudi dels motius negatius. Estan balancejades les classes? En cas negatiu, comenta (no ho implementis) que faries per balancejar el dataset."""

# Contem cadascuna de les categories de la variable objectiu
negativereason_counts = y_train.value_counts().reset_index()
negativereason_counts

#negativereason_counts.columns = ['negativereason', 'count']

# Creem el gràfic de barres
fig = px.bar(negativereason_counts, x='negativereason', y='count', title='Distribució de raons negatives')

# Mostrar el gràfic
fig.show()

"""iii)	Entrena un XGBoost (amb el valor 126 per reproduir resultats) que intenti predir el motiu d'un tuit negatiu. Implementa també un Random Search (entre 2 i 5 paràmetres per cada array del grid) per personalitzar els hiperparàmetres del model."""

# Mappegem perquè XGBoost necessita que la variable objectiu siga numèrica
mapping = {
    'Customer Service Issue': 0,
    'Late Flight': 1,
    "Can't Tell": 2,
    'Cancelled Flight': 3,
    'Lost Luggage': 4,
    'Bad Flight': 5,
    'Flight Booking Problems': 6,
    'Flight Attendant Complaints': 7,
    'longlines': 8,
    'Damaged Luggage': 9
}

# Mappegem i comprovem
y_test = y_test.map(mapping)
y_test.value_counts()

y_train = y_train.map(mapping)
y_train.value_counts()

import xgboost as xgb

# Seleccionem un conjunt de paràmetres
param_distributions = {
    'max_depth': [3, 5, 7],
    'subsample': [0.3, 0.5, 0.7],
    'learning_rate': [0.05, 0.15, 0.25],
    'n_estimators': [250, 500, 750]
}

clf = xgb.XGBClassifier(random_state=126)

# Fem el random search, fem 3 iteracions i 2 validacions creuades, perquè si no tardaríem massa temps
random_search = RandomizedSearchCV(estimator= clf, param_distributions=param_distributions,
                                   n_iter=3, cv=2, verbose=3, random_state=126, n_jobs=2)

# Entrenem el RandomizedSearchCV
random_search.fit(X_train, y_train)

# Mostrem el millor conjunt de paràmetres
print("Best parameters:", random_search.best_params_)

"""iv)	Quin encert té el model en funció de cada motiu, és a dir, quin percentatge d'encert té respecte a cadascun? (ex: el model encerta un 60% del tipus “Customer Service Issue”...)"""

import xgboost as xgb
# Fem el model XGB amb la millor combinació de paràmetres
best_xgb = xgb.XGBClassifier(
    subsample=0.7,
    n_estimators=500,
    max_depth=3,
    learning_rate=0.05,
    random_state=126
)

# Entrenem el model
best_xgb.fit(X_train, y_train)

# Fem les prediccions
y_pred = best_xgb.predict(X_test)

# Matriu de confusió
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title("Confusion Matrix for XGBoost")
plt.show()

# Reconvertim les variables numèriques en categòriques amb el mappejat invers
mapping = {0: 'Customer Service Issue', 1: 'Late Flight', 2: "Can't Tell", 3: 'Cancelled Flight', 4: 'Lost Luggage', 5: 'Bad Flight', 6: 'Flight Booking Problems', 7: 'Flight Attendant Complaints', 8: 'longlines', 9: 'Damaged Luggage'}
y_pred = [mapping[x] for x in y_pred]
y_test = [mapping[x] for x in y_test]

print("Classification Report for XGBoost \n")
print(classification_report(y_test, y_pred))

"""c)	Amb la llibreria scikit-learn, entrena una xarxa neuronal MLP que ens permeti classificar els tweets en positius i negatius amb un layer de 128 (potser, en funció del hardware has de reduir la dimensió o el conjunt de dades d'entrenament). Les èpoques i els altres hiperparàmetres necessaris els pots revisar i escollir llegint la documentació de la llibreria. Quin encert té el model? Computa les matrius de confusió sobre el dataset de train i el de test."""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score

# Emprem el MLPClassifier
mlp = MLPClassifier(
    # Fem 4 capes ocultes amb nombre descendent de neurones
    hidden_layer_sizes=(128, 64, 32, 16),
    max_iter=500,
    # Fem regularització L2
    alpha=0.01,
    # Taxa d'aprenentatge inicial
    learning_rate_init=0.001,  # Taxa d'aprenentatge inicial
    random_state=126,
    # Optimitzador Adam
    solver='adam',
    # Funció d'activació ReLU
    activation='relu'
)

# Entrenem el model
mlp.fit(X_train_cas2, y_train_cas2)

y_pred = mlp.predict(X_train_cas2)

cm = confusion_matrix(y_train_cas2, y_pred, labels=model.classes_)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title("Confusion Matrix for MLP Train")
plt.show()

print("Classification Report for MLP Train:\n")
print(classification_report(y_train_cas1, y_pred, target_names=model.classes_))

# Prediccions amb el conjunt de prova
y_pred = mlp.predict(X_test_cas2)

cm = confusion_matrix(y_test_cas2, y_pred, labels=model.classes_)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title("Confusion Matrix for MLP Test")
plt.show()

print("Classification Report for MLP Test:\n")
print(classification_report(y_test_cas1, y_pred, target_names=model.classes_))

"""d)	A partir dels models entrenats en les activitats 3a i 3b, on tenim un model que ens serveix per analitzar el sentiment dels tuits i un altre que ens ajuda a saber el perquè d'un tuit negatiu, crea una funció que a partir d'un tuit, ens digui si és positiu o negatiu i en cas de negatiu que ens intenti comentar el perquè.
Es recomana passar com a paràmetres els models a la funció o crear variables globals.

"""

# Fem la funció que uneix els models 3a i 3b, no he posat els models com paràmetres, ja que ja he seleccionat el models que millor funcionen
def tweet_classifactor(tweet):
  # tokenitzem i eliminem puntuacions
  tokens = tokenize_and_remove_punctuations(tweet)
  # Eliminem stop words
  tokens = remove_stop_words(tokens)
  # Lemmatització
  tokens = function_lemmatizer(tokens)
  # Separem els tokens amb un espai per a què quede una string
  sentence = ' '.join(tokens)
  # Transormem l'oració a una matriu TF-IDF
  transform = v.transform([sentence])
  # Posem la matriu en forma d'array
  transform = transform.toarray()
  # Fem la predicció de positiu o negatiu
  y_pred = best_svc.predict(transform)
  # Fem un condicional
  if y_pred == 'negative':
    # Si es negatiu apliquem el segon model
    y_pred2 = best_xgb.predict(transform)
    # mappegem i convertim el resultat numèric per la categoria corresponent
    mapping = {0: 'Customer Service Issue', 1: 'Late Flight', 2: "Can't Tell", 3: 'Cancelled Flight', 4: 'Lost Luggage', 5: 'Bad Flight', 6: 'Flight Booking Problems', 7: 'Flight Attendant Complaints', 8: 'longlines', 9: 'Damaged Luggage'}
    y_pred2 = [mapping[x] for x in y_pred2]
    return y_pred[0], y_pred2[0]
  else:
    return y_pred[0]


# Fem proves, a veure si funciona correctament
tweet_classifactor("The Customer Service person was awful, he didn't help us")

tweet_classifactor("The flight with @Delta was amazing, thanks")

tweet_classifactor("We arrived very late to our destination")

# Fem un sample de 20 mostres aleatòries per veure com predeix els nostres models integrats en la funció creada
data_sample = data2['text'].sample(n=20, random_state=33)

data_sample = pd.DataFrame(data_sample)

data_sample['prediction'] = data_sample['text'].apply(tweet_classifactor)

data_sample = data_sample.merge(data[['airline_sentiment', 'negativereason']], left_index=True, right_index=True)

data_sample